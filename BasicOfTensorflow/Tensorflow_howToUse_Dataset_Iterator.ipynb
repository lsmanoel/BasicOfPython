{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_howToUse_Dataset_Iterator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsmanoel/BasicOfPython/blob/master/BasicOfTensorflow/Tensorflow_howToUse_Dataset_Iterator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ol5S62irx7FG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Efficient approach of Datasets e Iterators with Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "UciuiuV2wXmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A revision of [towardsdatascience.com/how-to-use-dataset-in-tensorflow](https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428)\n",
        "\n",
        "Oficial dataset guide: [www.tensorflow.org/guide/datasets](https://www.tensorflow.org/guide/datasets)\n",
        "\n",
        "About Data Input Pipeline Performance: [tensorflow.org/guide/performance/datasets](https://www.tensorflow.org/guide/performance/datasets)"
      ]
    },
    {
      "metadata": {
        "id": "W-TWdVo0SX1D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **feed-dict** is a slowest possible way to pass information to Tensorflow. This way to feed the model in Tensorflow must be avoided. The fast way to feed data into the models is to use an input pipeline. The Tensorflow has a built-in API called Dataset to use fast pipelines to feed our models."
      ]
    },
    {
      "metadata": {
        "id": "edvqvUe7pDe0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Pipelining to performance of GPU and CPU integrated system:**\n",
        "\n",
        "Pipelining overlaps the preprocessing and model execution of a training step. While the accelerator is performing training step N, the CPU is preparing the data for step N+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract and transform the data.\n",
        "\n",
        "Without pipelining, the CPU and the GPU/TPU sit idle much of the time:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/datasets_without_pipelining.png)\n",
        "\n",
        "With pipelining, idle time diminishes significantly:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/datasets_with_pipelining.png)\n",
        "\n",
        "The **tf.data** API provides a software pipelining mechanism through the **tf.data**.Dataset.prefetch transformation, which can be used to decouple the time data is produced from the time it is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. Thus, to achieve the pipelining effect illustrated above, you can add prefetch(1) as the final transformation to your dataset pipeline (or prefetch(n) if a single training step consumes n elements).\n",
        "\n",
        "Source:  [tensorflow.org/guide/performance/datasets](https://www.tensorflow.org/guide/performance/datasets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1HdwI8HArn5a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.batch(batch_size=FLAGS.batch_size)\n",
        "dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_UzvJQQVrmMF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Parallelize Data Transformation:**\n",
        "\n",
        "When preparing a batch, input elements may need to be pre-processed. To this end, the tf.data API offers the **tf.data.Dataset.map** transformation, which applies a user-defined function (for example, parse_fn from the running example) to each element of the input dataset. Because input elements are independent of one another, **the pre-processing can be parallelized across multiple CPU cores**. To make this possible, the map transformation provides the **num_parallel_calls** argument to specify the level of parallelism. For example, the following diagram illustrates the effect of s**etting num_parallel_calls=2** to the map transformation:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/datasets_parallel_map.png)\n",
        "\n",
        "Choosing the best value for the num_parallel_calls argument depends on your hardware, characteristics of your training data (such as its size and shape), the cost of your map function, and what other processing is happening on the CPU at the same time; a simple heuristic is to use the number of available CPU cores. For instance, if the machine executing the example above had 4 cores, it would have been more efficient to set num_parallel_calls=4. On the other hand, setting num_parallel_calls to a value much greater than the number of available CPUs can lead to inefficient scheduling, resulting in a slowdown.\n",
        "\n",
        "Source:  [tensorflow.org/guide/performance/datasets](https://www.tensorflow.org/guide/performance/datasets)\n"
      ]
    },
    {
      "metadata": {
        "id": "wQPNSTvtrdSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(map_func=parse_fn, num_parallel_calls=FLAGS.num_parallel_calls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJEfELrkUbhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing Data"
      ]
    },
    {
      "metadata": {
        "id": "Odx2Dw34UyKB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To feed the model with a numpy array:"
      ]
    },
    {
      "metadata": {
        "id": "URvX99cbSHws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# random vector of 20 elements with shape (32, 32, 4):\n",
        "nElements = 20\n",
        "input_x = np.random.sample((nElements, 32, 32, 4))\n",
        "\n",
        "# create the dataset:\n",
        "dataset = tf.data.Dataset.from_tensor_slices(input_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JT14YlUYV0rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data divided into features and labels:"
      ]
    },
    {
      "metadata": {
        "id": "QY0ynAvMV0EM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "nElements = 20\n",
        "x_data = np.random.sample((nElements, 32, 32, 4)) #features\n",
        "y_data = np.random.sample((nElements, 1)) #label\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_data,y_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QEynnWF5XrfO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use placeholders to dynamically change the data inside the Dataset:"
      ]
    },
    {
      "metadata": {
        "id": "FU3mZqUhX90z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "x_data = tf.placeholder(tf.float32, shape=[None, 32, 32, 4])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6M25zX7Z8h0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To feed from generators:"
      ]
    },
    {
      "metadata": {
        "id": "3L6gLaH_YZ2g",
        "colab_type": "code",
        "outputId": "2f130e68-3565-455a-85f3-1d33d0d08d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# from generator\n",
        "sequence = np.array([[[1],[2],[3]],\n",
        "                     [[4],[5],[6]],\n",
        "                     [[7],[8],[9]]])\n",
        "\n",
        "# sequence = np.array([[1],[3],[2]])\n",
        "# sequence = np.arange(4)\n",
        "def generator():\n",
        "    for row in sequence:\n",
        "      for x in row:\n",
        "        yield x*x\n",
        "\n",
        "dataset = tf.data.Dataset().batch(1).from_generator(generator,\n",
        "                                                    output_types= tf.int64, \n",
        "                                                    output_shapes=(tf.TensorShape(None)))\n",
        "\n",
        "iter = dataset.make_initializable_iterator()\n",
        "x = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(iter.initializer)\n",
        "    for i in range(sequence.shape[0]*sequence.shape[1]*sequence.shape[2]):\n",
        "      print(\"iter:{} => {}\".format(i+1, sess.run(x)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:1 => [1]\n",
            "iter:2 => [4]\n",
            "iter:3 => [9]\n",
            "iter:4 => [16]\n",
            "iter:5 => [25]\n",
            "iter:6 => [36]\n",
            "iter:7 => [49]\n",
            "iter:8 => [64]\n",
            "iter:9 => [81]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WvmGTs9ygtab",
        "colab_type": "code",
        "outputId": "dd37d8c0-a23e-4fe4-e63b-61d120d9eba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sequence.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "kDG3KTZlsdz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To read a csv file into a dataset:"
      ]
    },
    {
      "metadata": {
        "id": "4wb9wU0fq4u5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "bd52e775-afd8-4cea-9c75-4f137c9f9463"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NTbStOhhqIYT",
        "colab_type": "code",
        "outputId": "d758a7b4-cdcc-42e7-e9af-2bf1fed96f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# load a csv\n",
        "CSV_PATH = 'gdrive/My Drive/Colab Notebooks/AprendizagemNaoSupervisionada/self-organizingMaps/datasets/wines.csv'\n",
        "dataset = tf.data.experimental.make_csv_dataset(CSV_PATH, batch_size=32)\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "next = iter.get_next()\n",
        "print(next) # next is a dict with key=columns names and value=column data\n",
        "inputs, labels = next['Class'], next['Alcohol']\n",
        "\n",
        "with  tf.Session() as sess:\n",
        "    sess.run([inputs, labels])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('Class', <tf.Tensor 'IteratorGetNext_1:3' shape=(32,) dtype=int32>), ('Alcohol', <tf.Tensor 'IteratorGetNext_1:2' shape=(32,) dtype=float32>), ('Malic acid', <tf.Tensor 'IteratorGetNext_1:8' shape=(32,) dtype=float32>), (' Ash', <tf.Tensor 'IteratorGetNext_1:1' shape=(32,) dtype=float32>), (' Alcalinity of ash', <tf.Tensor 'IteratorGetNext_1:0' shape=(32,) dtype=float32>), ('Magnesium', <tf.Tensor 'IteratorGetNext_1:7' shape=(32,) dtype=int32>), ('Total phenols', <tf.Tensor 'IteratorGetNext_1:13' shape=(32,) dtype=float32>), ('Flavanoids', <tf.Tensor 'IteratorGetNext_1:5' shape=(32,) dtype=float32>), ('Nonflavanoid phenols', <tf.Tensor 'IteratorGetNext_1:9' shape=(32,) dtype=float32>), ('Proanthocyanins', <tf.Tensor 'IteratorGetNext_1:11' shape=(32,) dtype=float32>), ('Color intensity', <tf.Tensor 'IteratorGetNext_1:4' shape=(32,) dtype=float32>), ('Hue', <tf.Tensor 'IteratorGetNext_1:6' shape=(32,) dtype=float32>), ('OD280/OD315 of diluted wines', <tf.Tensor 'IteratorGetNext_1:10' shape=(32,) dtype=float32>), ('Proline', <tf.Tensor 'IteratorGetNext_1:12' shape=(32,) dtype=int32>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VPC5silms5cM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Iterator\n",
        "\n",
        "The iterator will give us the ability to iterate through the dataset and retrieve the real values of the data.\n",
        "\n",
        "There exist four types of iterators:\n",
        "\n",
        "\n",
        "\n",
        "*   **One Shot:** It can iterate once through a dataset, you cannot feed any value to it.\n",
        "*   **Initializable:** You can dynamically change calling its initializer operation and passing the new data with feed_dict . It’s basically a bucket that you can fill with stuff.\n",
        "*   **Reinitializable: ** It can be initialised from different Dataset. Very useful when you have a training dataset that needs some additional transformation, eg. shuffle, and a testing dataset. It’s like using a tower crane to select a different container.\n",
        "*   **Feedable:** It can be used to select with iterator to use. Following the previous example, it’s like a tower crane that selects which tower crane to use to select which container to take. In my opinion is useless.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g9Kmf3wKuQ5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One Shot:**"
      ]
    },
    {
      "metadata": {
        "id": "852KbIy1uboB",
        "colab_type": "code",
        "outputId": "247d6f73-8e59-460d-e3ae-f5aeaae93f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_array = np.arange(3)\n",
        "# make a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(input_array)\n",
        "\n",
        "# create the iterator\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "x = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run(x), sess.run(x), sess.run(x))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1UfBDAEvZg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initializable Iterator:**\n",
        "\n",
        "Create a dataset with a placeholder to build a dynamic dataset in which we can change the data source at runtime. The placeholder need be initialize by feed-dict mechanism. This time we call **make_initializable_iterator** method from [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
        "\n",
        "Then, inside the sess scope, we run the initializer operation in order to pass our data, in this case a random numpy array:"
      ]
    },
    {
      "metadata": {
        "id": "s00gMHi7wLbl",
        "colab_type": "code",
        "outputId": "208dbba8-1bae-423d-8676-08437e0dec2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# using a placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 4])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "\n",
        "nElements = 20\n",
        "input_x = np.random.sample((nElements, 32, 32, 4))\n",
        "\n",
        "iter = dataset.make_initializable_iterator() # create the iterator\n",
        "y = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # feed the placeholder with data\n",
        "    sess.run(iter.initializer, feed_dict={x: input_x}) \n",
        "    print(sess.run(y).shape)\n",
        "    print(sess.run(y).shape)\n",
        "    print(sess.run(y).shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 4)\n",
            "(32, 32, 4)\n",
            "(32, 32, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yreNtONb39W2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A real common scenario (train set and a test set):"
      ]
    },
    {
      "metadata": {
        "id": "emjmX9uA0qFL",
        "colab_type": "code",
        "outputId": "1d206078-1762-4fc1-91e3-fca0770e1c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
        "\n",
        "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
        "\n",
        "# initializable iterator to switch between dataset\n",
        "iter = dataset.make_initializable_iterator()\n",
        "features, labels = iter.get_next()\n",
        "\n",
        "EPOCHS = 10\n",
        "with tf.Session() as sess:\n",
        "#     initialise iterator with train data\n",
        "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
        "  \n",
        "    for _ in range(EPOCHS):\n",
        "        sess.run([features, labels])\n",
        "        \n",
        "#     switch to test data\n",
        "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
        "    print(sess.run([features, labels]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqzJg9Au5TQp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reinitializable Iterator:**\n",
        "\n",
        "The concept is similar to before, we want to dynamic switch between data. But instead of feed new data to the same dataset, we switch dataset. As before, we want to have a train dataset and a test dataset"
      ]
    },
    {
      "metadata": {
        "id": "M_OGvSL45l7_",
        "colab_type": "code",
        "outputId": "b359c274-daf3-4dfc-d445-6d020da6d46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# making fake data using numpy\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
        "\n",
        "# create two datasets, one for training and one for test\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "\n",
        "# create a iterator of the correct shape and type\n",
        "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                           train_dataset.output_shapes)\n",
        "\n",
        "# create the initialisation operations\n",
        "train_init_op = iter.make_initializer(train_dataset)\n",
        "test_init_op = iter.make_initializer(test_dataset)\n",
        "\n",
        "# get the next element as before\n",
        "features, labels = iter.get_next()\n",
        "\n",
        "# Reinitializable iterator to switch between Datasets\n",
        "EPOCHS = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(train_init_op) # switch to train dataset\n",
        "    \n",
        "    for _ in range(EPOCHS):\n",
        "        sess.run([features, labels])\n",
        "        \n",
        "    sess.run(test_init_op) # switch to val dataset\n",
        "    print(sess.run([features, labels]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.77451975, 0.54779441]), array([0.65103328])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZfVlOOdk6gH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feedable Iterator:**\n",
        "\n",
        "This is very similar to the **Reinitializable Iterator**, but instead of switch between datasets, it switch between iterators.\n",
        "[tensorflow.org/programmers_guide/datasets#creating_an_iterator](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator)"
      ]
    },
    {
      "metadata": {
        "id": "qP_m3pd669UU",
        "colab_type": "code",
        "outputId": "44988d3a-8c70-4e0d-fda4-7dc684c81d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# making fake data using numpy\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
        "\n",
        "# create placeholder\n",
        "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "# create two datasets, one for training and one for test\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create the iterators from the dataset\n",
        "train_iterator = train_dataset.make_initializable_iterator()\n",
        "test_iterator = test_dataset.make_initializable_iterator()\n",
        "\n",
        "# same as in the doc: \n",
        "# https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\n",
        "handle = tf.placeholder(tf.string, shape=[])\n",
        "iter = tf.data.Iterator.from_string_handle(\n",
        "    handle, train_dataset.output_types, train_dataset.output_shapes)\n",
        "next_elements = iter.get_next()\n",
        "\n",
        "# feedable iterator to switch between iterators\n",
        "EPOCHS = 10\n",
        "with tf.Session() as sess:\n",
        "    train_handle = sess.run(train_iterator.string_handle())\n",
        "    test_handle = sess.run(test_iterator.string_handle())\n",
        "    \n",
        "    # initialise iterators. \n",
        "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
        "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
        "    \n",
        "    for _ in range(EPOCHS):\n",
        "        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})\n",
        "        print(x, y)\n",
        "        \n",
        "    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})\n",
        "    print(x,y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.40486744 0.09454836] [0.45837525]\n",
            "[0.40568185 0.25635305] [0.9361035]\n",
            "[0.18568978 0.8688876 ] [0.88782215]\n",
            "[0.03910578 0.6690394 ] [0.6457239]\n",
            "[0.30962598 0.42874312] [0.72969776]\n",
            "[0.46682042 0.13815157] [0.23211609]\n",
            "[0.22990501 0.68973386] [0.10259616]\n",
            "[0.39459708 0.09110982] [0.9414525]\n",
            "[0.6679728 0.4056337] [0.74841356]\n",
            "[0.7971969  0.94630957] [0.40634608]\n",
            "[0.59526926 0.2725911 ] [0.70964456]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSFOs4UB7puG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Consuming data:**\n",
        "\n",
        "In order to pass the data to a model we have to just pass the tensors generated from **get_next()**"
      ]
    },
    {
      "metadata": {
        "id": "T-wYO5W17o2J",
        "colab_type": "code",
        "outputId": "4d4605c3-a73d-49a8-b77a-4dcf5086d3b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# using two numpy arrays\n",
        "features, labels = (np.array([np.random.sample((100,2))]), \n",
        "                    np.array([np.random.sample((100,1))]))\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
        "\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "# make a simple model\n",
        "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
        "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
        "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
        "\n",
        "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "EPOCHS = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(EPOCHS):\n",
        "        _, loss_value = sess.run([train_op, loss])\n",
        "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.5263\n",
            "Iter: 1, Loss: 0.5098\n",
            "Iter: 2, Loss: 0.4936\n",
            "Iter: 3, Loss: 0.4778\n",
            "Iter: 4, Loss: 0.4623\n",
            "Iter: 5, Loss: 0.4471\n",
            "Iter: 6, Loss: 0.4323\n",
            "Iter: 7, Loss: 0.4179\n",
            "Iter: 8, Loss: 0.4038\n",
            "Iter: 9, Loss: 0.3901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xr40qN7X8Zt7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Useful Stuff"
      ]
    },
    {
      "metadata": {
        "id": "QGhY5eef8dzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Batch:**\n",
        "\n",
        "The Dataset API has the method **batch(BATCH_SIZE)** that automatically batches the dataset with the provided size.\n"
      ]
    },
    {
      "metadata": {
        "id": "MCosFB4G9CEH",
        "colab_type": "code",
        "outputId": "031eb50e-5f64-4873-bed0-bd7df72126ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# BATCHING\n",
        "BATCH_SIZE = 4\n",
        "x = np.random.sample((100,2))\n",
        "# make a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
        "\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "y = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run(y)) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.19498597 0.07937528]\n",
            " [0.34385483 0.55348209]\n",
            " [0.7791454  0.13523891]\n",
            " [0.21017553 0.11101988]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ay4xp9sgjbF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Repeat:**\n",
        "\n",
        "Using **.repeat()** we can specify the number of times we want the dataset to be iterated. If no parameter is passed it will loop forever, usually is good to just loop forever and directly control the number of epochs with a standard loop.\n"
      ]
    },
    {
      "metadata": {
        "id": "2_ZU6tUpml02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "46daf946-e3c2-46f0-c262-7cd4de4d9616"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#Random Sequence Generator\n",
        "\n",
        "# tf.data.Dataset.range(9) -> Creates a Dataset of a step-separated range of values.\n",
        "\n",
        "# .shuffle(buffer_size=10) -> shuffle 10 elements of dataset\n",
        "# shuffle(\n",
        "#     buffer_size,\n",
        "#     seed=None,\n",
        "#     reshuffle_each_iteration=None -> Default = True\n",
        "# )\n",
        "\n",
        "\n",
        "# .repeat(None) or .repeat(-1) -> Repeat indefinitely \n",
        "\n",
        "n = tf.data.Dataset.range(9).shuffle(buffer_size=10).repeat(None).make_one_shot_iterator().get_next()\n",
        "with tf.Session() as sess:\n",
        "  [print(sess.run(n)) for _ in range(10)]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "8\n",
            "6\n",
            "0\n",
            "5\n",
            "2\n",
            "3\n",
            "1\n",
            "4\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "61mkQ14ZkZ0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Shuffle:**\n",
        "\n",
        "We can shuffle the Dataset by using the method **shuffle() ** that shuffles the dataset by default every epoch.\n",
        "\n",
        "We can also set the parameter **buffer_size** , a fixed size buffer from which the next element will be uniformly chosen from."
      ]
    },
    {
      "metadata": {
        "id": "8eTAyzB-jbwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "25744f45-402c-4e7d-b9fe-7be3abee636d"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# BATCHING\n",
        "BATCH_SIZE = 2 # 6 elements and 3 BATCHS of size 2\n",
        "x = np.array([[0],[1],[2],[3],[4],[5]])\n",
        "\n",
        "# make a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "dataset = dataset.shuffle(buffer_size=2)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "el = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"Without repeat:\")\n",
        "    print(\"--------------\")\n",
        "    print(\"First Batch:\\n\",  sess.run(el))\n",
        "    print(\"Second Batch:\\n\", sess.run(el))\n",
        "    print(\"Third Batch:\\n\",  sess.run(el))\n",
        "    print(\"\\n\")\n",
        "    \n",
        "# WITH REPEAT:\n",
        "# make a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "dataset = dataset.shuffle(buffer_size=2)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.repeat(None)\n",
        "\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "el = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"With repeat:\")\n",
        "    print(\"--------------\")\n",
        "    print(\"First Batch:\\n\",  sess.run(el))\n",
        "    print(\"Second Batch:\\n\", sess.run(el))\n",
        "    print(\"Third Batch:\\n\",  sess.run(el))\n",
        "    print(\"...\")\n",
        "    print(\"First Batch:\\n\",  sess.run(el))\n",
        "    print(\"Second Batch:\\n\", sess.run(el))\n",
        "    print(\"Third Batch:\\n\",  sess.run(el))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without repeat:\n",
            "--------------\n",
            "First Batch:\n",
            " [[0]\n",
            " [2]]\n",
            "Second Batch:\n",
            " [[1]\n",
            " [4]]\n",
            "Third Batch:\n",
            " [[3]\n",
            " [5]]\n",
            "\n",
            "\n",
            "With repeat:\n",
            "--------------\n",
            "First Batch:\n",
            " [[0]\n",
            " [2]]\n",
            "Second Batch:\n",
            " [[3]\n",
            " [1]]\n",
            "Third Batch:\n",
            " [[5]\n",
            " [4]]\n",
            "...\n",
            "First Batch:\n",
            " [[1]\n",
            " [0]]\n",
            "Second Batch:\n",
            " [[2]\n",
            " [4]]\n",
            "Third Batch:\n",
            " [[3]\n",
            " [5]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Auv5vpoY5wwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Map\n",
        "\n",
        "You can apply a custom function to each member of a dataset using the **map** method. In the following example we multiply each element by two:"
      ]
    },
    {
      "metadata": {
        "id": "4k0UCKS65tq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "42775c01-8651-4e8a-8316-e789f2c0fd05"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "x = np.array([[1],[2],[3],[4]])\n",
        "# make a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "\n",
        "# MAP:\n",
        "dataset = dataset.map(lambda x: x*2)\n",
        "\n",
        "iter = dataset.make_one_shot_iterator()\n",
        "el = iter.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#     this will run forever\n",
        "        for _ in range(len(x)):\n",
        "            print(sess.run(el))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n",
            "[4]\n",
            "[6]\n",
            "[8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8NkchsKb8ine",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Full example"
      ]
    },
    {
      "metadata": {
        "id": "VXKUJt_M8lVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initializable iterator:**"
      ]
    },
    {
      "metadata": {
        "id": "qe42A8hM6b6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "0ceb2e5c-017e-47bb-8102-baced347fece"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Wrapping all together -> Switch between train and test set using Initializable iterator\n",
        "EPOCHS = 10\n",
        "\n",
        "# create a placeholder to dynamically switch between batch sizes\n",
        "BATCH_SIZE = 4\n",
        "batch_size = tf.placeholder(tf.int64)\n",
        "\n",
        "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
        "\n",
        "# using two numpy arrays\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
        "\n",
        "iter = dataset.make_initializable_iterator()\n",
        "features, labels = iter.get_next()\n",
        "\n",
        "# make a simple model\n",
        "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
        "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
        "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
        "\n",
        "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # initialise iterator with train data\n",
        "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
        "    \n",
        "    print('Training...')\n",
        "    for i in range(EPOCHS):\n",
        "        tot_loss = 0\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            _, loss_value = sess.run([train_op, loss])\n",
        "            tot_loss += loss_value\n",
        "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / BATCH_SIZE))\n",
        "    # initialise iterator with test data\n",
        "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
        "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Iter: 0, Loss: 0.1061\n",
            "Iter: 1, Loss: 0.1186\n",
            "Iter: 2, Loss: 0.0487\n",
            "Iter: 3, Loss: 0.1480\n",
            "Iter: 4, Loss: 0.1530\n",
            "Iter: 5, Loss: 0.1027\n",
            "Iter: 6, Loss: 0.0839\n",
            "Iter: 7, Loss: 0.1120\n",
            "Iter: 8, Loss: 0.0325\n",
            "Iter: 9, Loss: 0.0894\n",
            "Test Loss: 0.108598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FzGrGLOW8onL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reinitializable Iterator:**"
      ]
    },
    {
      "metadata": {
        "id": "UMNDS1Qa8zvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8092eaed-0b35-4ef2-b1a9-27a5600118ed"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
        "EPOCHS = 10\n",
        "\n",
        "# create a placeholder to dynamically switch between batch sizes\n",
        "BATCH_SIZE = 4\n",
        "batch_size = tf.placeholder(tf.int64)\n",
        "\n",
        "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it\n",
        "\n",
        "# using two numpy arrays\n",
        "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
        "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
        "\n",
        "# create a iterator of the correct shape and type\n",
        "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                           train_dataset.output_shapes)\n",
        "features, labels = iter.get_next()\n",
        "# create the initialisation operations\n",
        "train_init_op = iter.make_initializer(train_dataset)\n",
        "test_init_op = iter.make_initializer(test_dataset)\n",
        "\n",
        "# make a simple model\n",
        "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
        "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
        "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
        "\n",
        "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # initialise iterator with train data\n",
        "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
        "    \n",
        "    print('Training...')\n",
        "    for i in range(EPOCHS):\n",
        "        tot_loss = 0\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            _, loss_value = sess.run([train_op, loss])\n",
        "            tot_loss += loss_value\n",
        "            \n",
        "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / BATCH_SIZE))\n",
        "    \n",
        "    # initialise iterator with test data\n",
        "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
        "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Iter: 0, Loss: 0.3457\n",
            "Iter: 1, Loss: 0.2616\n",
            "Iter: 2, Loss: 0.2503\n",
            "Iter: 3, Loss: 0.2147\n",
            "Iter: 4, Loss: 0.1785\n",
            "Iter: 5, Loss: 0.1448\n",
            "Iter: 6, Loss: 0.1349\n",
            "Iter: 7, Loss: 0.1093\n",
            "Iter: 8, Loss: 0.0967\n",
            "Iter: 9, Loss: 0.0935\n",
            "Test Loss: 0.085259\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}